{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on AixCaliBuHA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before going any further, please note that you first should read the tutorial on `ebcpy` ([Link](https://git.rwth-aachen.de/EBC/EBC_all/Python/ebcpy)). The basic data types are explained there. Also, an example for the simulation API is made hinting on AixCaliBuHa and why you may need it.*\n",
    "\n",
    "### What is AixCaliBuHA?\n",
    "\n",
    "**Aix** (from French Aix-la-Chapelle) **Cali**bration for **Bu**ilding and **H**V**A**C Systems\n",
    "\n",
    "This framework attempts to make the process of calibrating models used in building\n",
    "and HVAC systems easier. Different sub packages help with the underlying tasks of:\n",
    "\n",
    "- Performing a **Sensitivity Analysis** to discover tuner parameters for the calibration\n",
    "- **Calibration** of given model based on the tuner parameters, the calibration classes and specified goals to evaluate the objective function of the underlying optimization\n",
    "\n",
    "### Why Calibration\n",
    "\n",
    "When modelling Building and HVAC-Systems, one wants to make valid statements about the real system. A model which represents some real-world process in a sufficient manner (e.g. in terms of supply temperature) may be useful to makes such statements. Model parameters have to be tuned so that some simulation output matches the output in the real world (e.g. some measurement). \n",
    "- **Which parameters to tune?** You as a modeler may already know important parameters that influence your model regarding a targeted output. However, performing a **sensitivity analysis** may be helpful to quantify which parameters are important.\n",
    "- **How to tune?** Manual tuning is most often still state of the art. However it is very inefficient. Therefore, the appraoch in this framework is the combination of mathematical **optimization** for calcuation of the next **tuner parameter** value and Simulation APIs for **automation of the optimization**.\n",
    "\n",
    "### Content of this Tutorial:\n",
    " 1. [The Basics: Tuner Paramateres, Goals and Calibration-Classes](#basics)\n",
    " 2. [Sensitivity Analysis: Get to know your model parameters](#sensanalysis)\n",
    " 3. [Modelica-Calibration: Getting started on calibration](#single_cal)\n",
    " 4. [Advanced Calibration: Multiple Classes, kwargs, solvers and best practices](#adv_cal)\n",
    "  1. [Multiple-Classes Calibration](#mult_cal)\n",
    "  2. [kwargs - Settings of the Calibrator](#kwargs_cal)\n",
    "  3. [Solver Options](#kwargs_solver)\n",
    "  4. [Best Practices](#best_practices)\n",
    "  5. [Visualization: The different plots explained](#visual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basics'></a>\n",
    "## The Basics: Tuner Paramaters, Goals and Calibration Classes\n",
    "\n",
    "Partly introduced in `ebcpy`, we will briefly explain the underlying `data_types`.\n",
    "\n",
    "### Tuner Parameters:\n",
    "\n",
    "All model parameters used either in a Sensitivity Analysis, an Optimization or a Calibration are at some point tuned, thus are tuner paramteres.\n",
    "Basically, a tuner parameter has a name (string), an initial value and minimal/maximal values (floats). For efficient optimization, the values are internally normalized to the range 0..1 (if min/max values (bounds) are given. This way different units (e.g. Temperature / K and Pressure / Pa) behave the same during optimization. Some solvers require boundaries some not. Initial values are not required for some global or stochastic solvers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 initial_value  min    max  scale\n",
      "names                                            \n",
      "HeatExchanger.G             25    0    300    300\n",
      "HeatExchanger.C           3000    0  10000  10000\n"
     ]
    }
   ],
   "source": [
    "from aixcalibuha import TunerParas\n",
    "\n",
    "tuner_paras = TunerParas(\n",
    "        # A list with the names of the parameters in the model.\n",
    "        names=[\"HeatExchanger.G\", \"HeatExchanger.C\"], \n",
    "        # List with initial values as floats\n",
    "        initial_values=[25, 3000],\n",
    "        # List with tuples. First item is minimal, second item maximal value\n",
    "        bounds=[(0, 300), (0, 10000)])\n",
    "\n",
    "# Lets look at the object (a DataFrame is internally used)\n",
    "print(tuner_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 initial_value  min  max  scale\n",
      "names                                          \n",
      "HeatExchanger.G             25    0  400    400\n"
     ]
    }
   ],
   "source": [
    "# Most functions (scale, descale, etc.) are used internally for automatic calibration or similiar. \n",
    "# You may find useful:\n",
    "tuner_paras.set_value(\"HeatExchanger.G\", \"max\", 400)\n",
    "tuner_paras.remove_names([\"HeatExchanger.C\"])\n",
    "print(tuner_paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals:\n",
    "\n",
    "Goals are used to evaluate the difference between measured and simulated data. You may want to calibrate your model based on multiple values, e.g. power consumption, supply temperature etc.\n",
    "As mentioned in `ebcpy`, we use our own **multi index `DataFrame`**. Here with the row names `Variables` and `Tags`. Looking at `Goals`, we therefore have the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables           measured_T_heater measured_T_heater_1\n",
      "Tags                              raw                 raw\n",
      "Time                                                     \n",
      "2020-03-01 15:10:49        313.165863          293.173126\n",
      "2020-03-01 15:10:50        312.090271          293.233002\n",
      "2020-03-01 15:10:51        312.090027          293.385925\n",
      "2020-03-01 15:10:52        312.109436          293.589233\n",
      "2020-03-01 15:10:53        312.130341          293.819611\n",
      "...                               ...                 ...\n",
      "2020-03-01 16:10:45        312.790192          302.844788\n",
      "2020-03-01 16:10:46        312.790192          302.844788\n",
      "2020-03-01 16:10:47        312.790192          302.844788\n",
      "2020-03-01 16:10:48        312.790192          302.844788\n",
      "2020-03-01 16:10:49        312.790192          302.844788\n",
      "\n",
      "[3601 rows x 2 columns]\n",
      "\n",
      "    Class for one or multiple goals. Used to evaluate the\n",
      "    difference between current simulation and measured data\n",
      "\n",
      "    :param (ebcpy.data_types.TimeSeriesData, pd.DataFrame) meas_target_data:\n",
      "        The dataset of the measurement. It acts as a point of reference\n",
      "        for the simulation output. If the dimensions of the given DataFrame and later\n",
      "        added simulation-data are not equal, an error is raised.\n",
      "        Has to hold all variables listed under the MEASUREMENT_NAME variable in the\n",
      "        variable_names dict.\n",
      "    :param dict variable_names:\n",
      "        A dictionary to construct the goals-DataFrame using pandas MultiIndex-Functionality.\n",
      "        The dict has to follow the structure.\n",
      "        variable_names = {VARIABLE_NAME: [MEASUREMENT_NAME, SIMULATION_NAME]}\n",
      "            - VARIABLE_NAME: A string which holds the actual name\n",
      "                of the variable you use as a goal.\n",
      "                E.g.: VARIABLE_NAME=\"Temperature_Condenser_Outflow\"\n",
      "            - MEASUREMENT_NAME: Is either a string or a tuple. Hold the name the variable\n",
      "                has inside the given meas_target_data. If you want to specify a tag you have\n",
      "                to pass a tuple, like: (MEASUREMENT_NAME, TAG_NAME). Else just pass a string.\n",
      "                E.g.: MEASUREMENT_NAME=\"HydraulicBench[4].T_Out\" or\n",
      "                      MEASUREMENT_NAME=(\"HydraulicBench[4].T_Out\", \"preprocessed\")\n",
      "            - SIMULATION_NAME is either a string or a tuple, just like MEASUREMENT_NAME.\n",
      "                E.g. (for Modelica): SIMULATION_NAME=\"HeatPump.Condenser.Vol.T\"\n",
      "        You may use a tuple instead of a list OR a dict\n",
      "        with key \"meas\" for measurement and key \"sim\" for simulation. These options may be\n",
      "        relevant for your own code readability.\n",
      "        E.g. variable_names = {VARIABLE_NAME: {\"meas\":MEASUREMENT_NAME,\n",
      "                                               \"sim\": SIMULATION_NAME}}\n",
      "\n",
      "    :param list weightings:\n",
      "        Values between 0 and 1 to account for multiple Goals to be evaluated.\n",
      "        If multiple goals are selected, and weightings is None, each\n",
      "        weighting will be equal to 1/(Number of goals).\n",
      "        The weighting is scaled so that the sum will equal 1.\n",
      "    \n",
      "Variables              T_heater  T_heater_1\n",
      "Tags                       meas        meas\n",
      "Time                                       \n",
      "2020-03-01 15:10:49  313.165863  293.173126\n",
      "2020-03-01 15:10:50  312.090271  293.233002\n",
      "2020-03-01 15:10:51  312.090027  293.385925\n",
      "2020-03-01 15:10:52  312.109436  293.589233\n",
      "2020-03-01 15:10:53  312.130341  293.819611\n",
      "...                         ...         ...\n",
      "2020-03-01 16:10:45  312.790192  302.844788\n",
      "2020-03-01 16:10:46  312.790192  302.844788\n",
      "2020-03-01 16:10:47  312.790192  302.844788\n",
      "2020-03-01 16:10:48  312.790192  302.844788\n",
      "2020-03-01 16:10:49  312.790192  302.844788\n",
      "\n",
      "[3601 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from ebcpy import data_types, preprocessing\n",
    "from aixcalibuha import Goals\n",
    "import os\n",
    "\n",
    "# First we load our measurement data as time series data\n",
    "path = os.path.join(os.getcwd(), \"data\", \"ref_result.hdf\")\n",
    "mtd = data_types.TimeSeriesData(path, key=\"test\")\n",
    "#mtd = preprocessing.build_average_on_duplicate_rows(mtd)\n",
    "print(mtd)\n",
    "# I refer to the docstring of the class for more information. If you have further questions, please raise an Issue.\n",
    "print(Goals.__doc__)\n",
    "\n",
    "# Recall from the doc-strings that the dict has the following structure:\n",
    "# variable_names = {VARIABLE_NAME: [MEASUREMENT_NAME, SIMULATION_NAME]}\n",
    "variable_names = {\"T_heater\": [\"measured_T_heater\", \"heater.heatPorts[1].T\"],\n",
    "                  \"T_heater_1\": [\"measured_T_heater_1\", \"heater1.heatPorts[1].T\"]}\n",
    "\n",
    "goals = Goals(meas_target_data=mtd,\n",
    "              variable_names=variable_names,\n",
    "              # Use weightings if one of the goals is more relevant than another. A weighted sum is applied,\n",
    "              # for example with [0.9, 0.1] the total objective will be = 0.9 * objective_goal_1 + 0.1 * objective_goal_2  \n",
    "              weightings=[0.3, 0.7])\n",
    "\n",
    "print(goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assume we've run a simulation and want to add the result data to our Goals object. (This is done automatically in the Calibration process.) First we load the simulation data and analyze it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables heater.heatPorts[1].T heater1.heatPorts[1].T\n",
      "Tags                        raw                    raw\n",
      "Time                                                  \n",
      "0.0                  313.165863             293.173126\n",
      "1.0                  310.787750             293.352448\n",
      "2.0                  310.796753             293.719055\n",
      "3.0                  310.870331             294.141754\n",
      "4.0                  310.936859             294.564545\n",
      "...                         ...                    ...\n",
      "596.0                307.777649             307.777649\n",
      "597.0                307.777649             307.777649\n",
      "598.0                307.777649             307.777649\n",
      "599.0                307.777649             307.777649\n",
      "600.0                307.777649             307.777649\n",
      "\n",
      "[601 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), \"data\", \"simTargetData.mat\")\n",
    "# The class data_types.TimeSeriesData can handle both *.hdf and *.mat files. \n",
    "# Latter one are the default result file format from Modelica simualtions.\n",
    "std = data_types.TimeSeriesData(path)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we may already expect some error. Why? Because our sim-target-data is using some `float`-based index but our measurement data is using `DatetimeIndex` as an index. We can solve this issue in two ways: Either convert `std` to a `DatetimeIndex` or `mtd` to a `float`-based index. Which way to choose? We agreed on the latter one, mainly because of efficiency.  \n",
    "`set_sim_target_data` is called every iteration, therefore the conversion would take place every iteration. Iteration here means that during the subsequent calibration process the simulation is called multiple times and this always calls afterwards the `set_sim_target_data` function. Thus, processing the `std` with the default `float`-based index saves the operation of converting the index type. In contrast, `mtd` index needs to be converted only once! To display your result in the end you may use the original index again, but for the calibration, the index of `std` is used. For the conversion, we offer a preprocessing function in `ebcpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError: Given sim_target_data is using Float64Index as an index, but the reference results (measured-data) was declared using the DatetimeIndex. Convert your measured-data index to solve this error.\n",
      " \n",
      "Variables    T_heater              T_heater_1            \n",
      "Tags             meas         sim        meas         sim\n",
      "Time                                                     \n",
      "0.0        313.165863  313.165863  293.173126  293.173126\n",
      "1.0        312.090271  310.787750  293.233002  293.352448\n",
      "2.0        312.090027  310.796753  293.385925  293.719055\n",
      "3.0        312.109436  310.870331  293.589233  294.141754\n",
      "4.0        312.130341  310.936859  293.819611  294.564545\n",
      "...               ...         ...         ...         ...\n",
      "3596.0     312.790192         NaN  302.844788         NaN\n",
      "3597.0     312.790192         NaN  302.844788         NaN\n",
      "3598.0     312.790192         NaN  302.844788         NaN\n",
      "3599.0     312.790192         NaN  302.844788         NaN\n",
      "3600.0     312.790192         NaN  302.844788         NaN\n",
      "\n",
      "[3601 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    goals.set_sim_target_data(sim_target_data=std)\n",
    "except IndexError as e:\n",
    "    print(\"IndexError:\", e)\n",
    "    \n",
    "path = os.path.join(os.getcwd(), \"data\", \"ref_result.hdf\")\n",
    "mtd = data_types.TimeSeriesData(path, key=\"test\")\n",
    "mtd.to_float_index()\n",
    "# Now we may construct our goals and set the simulation-target-data:\n",
    "goals = Goals(meas_target_data=mtd,\n",
    "              variable_names=variable_names,\n",
    "              weightings=[0.3, 0.7])\n",
    "goals.set_sim_target_data(std)\n",
    "# Let's look at the data:\n",
    "print(' ') # Just add an empty line\n",
    "print(goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see how the sim-target-data was added. In a calibration, each variable is internally evaluated using some statistical measure that you provide. But more on that in the part for calibration. Don't worry about the `NaN` values, we will handle them using the python-class `CalibrationClass`. They occur due to the fact that our measurement is 1 hour long, but the example simulation data only 10 minutes. Internally, `set_relevant_time_intervals` is used to just include the data in a specific interval (the interval is provided in the `CalibrationClass` object). \n",
    "You may play with the `statistical_measure`. The verbose option is used for the logger to better visualize how the weightings affect the end result and which goal is performing well and which not so much.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables    T_heater              T_heater_1            \n",
      "Tags             meas         sim        meas         sim\n",
      "Time                                                     \n",
      "0.0        313.165863  313.165863  293.173126  293.173126\n",
      "1.0        312.090271  310.787750  293.233002  293.352448\n",
      "2.0        312.090027  310.796753  293.385925  293.719055\n",
      "3.0        312.109436  310.870331  293.589233  294.141754\n",
      "4.0        312.130341  310.936859  293.819611  294.564545\n",
      "...               ...         ...         ...         ...\n",
      "596.0      312.790192  307.777649  302.844849  307.777649\n",
      "597.0      312.790192  307.777649  302.844849  307.777649\n",
      "598.0      312.790192  307.777649  302.844849  307.777649\n",
      "599.0      312.790192  307.777649  302.844849  307.777649\n",
      "600.0      312.790192  307.777649  302.844849  307.777649\n",
      "\n",
      "[601 rows x 4 columns]\n",
      "(1.0444056379002862, {0.3: 2.6869453543928863, 0.7: 0.3404600451177434})\n",
      "Help on class StatisticsAnalyzer in module ebcpy.utils.statistics_analyzer:\n",
      "\n",
      "class StatisticsAnalyzer(builtins.object)\n",
      " |  StatisticsAnalyzer(method, for_minimization=True)\n",
      " |  \n",
      " |  Class for calculation of the statistical measure based on the\n",
      " |  given method. Either instantiate the class and run\n",
      " |  StatisticsAnalyzer.calc(meas, sim), or go for direct calculation with\n",
      " |  StatisticsAnalyzer.calc_METHOD(meas, sim). Where METHOD stands for one\n",
      " |  of the available methods (see below).\n",
      " |  \n",
      " |  :param str method:\n",
      " |      One of the following:\n",
      " |          - MAE(Mean absolute error)\n",
      " |          - R2(coefficient of determination)\n",
      " |          - MSE (Mean squared error)\n",
      " |          - RMSE(root mean square error)\n",
      " |          - CVRMSE(variance of RMSE)\n",
      " |          - NRMSE(Normalized RMSE)\n",
      " |  :param Boolean for_minimization:\n",
      " |      Default True. To reduce (minimize) the error in given data,\n",
      " |      we either have to minimize or maximize the statistical measure.\n",
      " |      Example: R2 has to be maximized to minimize the error in data.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, method, for_minimization=True)\n",
      " |      Instantiate class parameters\n",
      " |  \n",
      " |  calc(self, meas, sim)\n",
      " |      Placeholder class before instantiating the class correctly.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  calc_cvrmse(meas, sim)\n",
      " |      Calculates the CVRMSE (variance of root mean square error)\n",
      " |      THIS IS A TEST\n",
      " |      for the given numpy array of measured and simulated data.\n",
      " |      \n",
      " |      :param np.array meas:\n",
      " |          Array with measurement data\n",
      " |      :param np.array sim:\n",
      " |          Array with simulation data\n",
      " |      :return: float CVRMSE:\n",
      " |          CVRMSE of the given data.\n",
      " |  \n",
      " |  calc_mae(meas, sim)\n",
      " |      Calculates the MAE (mean absolute error)\n",
      " |      for the given numpy array of measured and simulated data.\n",
      " |      \n",
      " |      :param np.array meas:\n",
      " |          Array with measurement data\n",
      " |      :param np.array sim:\n",
      " |          Array with simulation data\n",
      " |      :return: float MAE:\n",
      " |          MAE os the given data.\n",
      " |  \n",
      " |  calc_mse(meas, sim)\n",
      " |      Calculates the MSE (mean square error)\n",
      " |      for the given numpy array of measured and simulated data.\n",
      " |      \n",
      " |      :param np.array meas:\n",
      " |          Array with measurement data\n",
      " |      :param np.array sim:\n",
      " |          Array with simulation data\n",
      " |      :return: float MSE:\n",
      " |          MSE of the given data.\n",
      " |  \n",
      " |  calc_nrmse(meas, sim)\n",
      " |      Calculates the NRMSE (normalized root mean square error)\n",
      " |      for the given numpy array of measured and simulated data.\n",
      " |      \n",
      " |      :param np.array meas:\n",
      " |          Array with measurement data\n",
      " |      :param np.array sim:\n",
      " |          Array with simulation data\n",
      " |      :return: float NRMSE:\n",
      " |          NRMSE of the given data.\n",
      " |  \n",
      " |  calc_r2(meas, sim)\n",
      " |      Calculates the MAE (mean absolute error)\n",
      " |      for the given numpy array of measured and simulated data.\n",
      " |      \n",
      " |      :param np.array meas:\n",
      " |          Array with measurement data\n",
      " |      :param np.array sim:\n",
      " |          Array with simulation data\n",
      " |      :return: float MAE:\n",
      " |          R2 of the given data.\n",
      " |  \n",
      " |  calc_rmse(meas, sim)\n",
      " |      Calculates the RMSE (root mean square error)\n",
      " |      for the given numpy array of measured and simulated data.\n",
      " |      \n",
      " |      :param np.array meas:\n",
      " |          Array with measurement data\n",
      " |      :param np.array sim:\n",
      " |          Array with simulation data\n",
      " |      :return: float RMSE:\n",
      " |          RMSE of the given data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goals.set_relevant_time_intervals([(0, 600)])\n",
    "print(goals)\n",
    "print(goals.eval_difference(statistical_measure=\"NRMSE\", verbose=True))\n",
    "# Here is the list of all avaliable statistical measures:\n",
    "from ebcpy.utils.statistics_analyzer import StatisticsAnalyzer\n",
    "help(StatisticsAnalyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Classes:\n",
    "Last but not least we need an object to define in what time interval we want to calibrate our simulation model.\n",
    "We call this time interval **Calibration Class**, not to be confused with the **python class**. A `CalibrationClass` contains everything we need to run the calibration.  \n",
    "We need to ask ourselves: **What type of `CalibrationClass` are we talking about?**  \n",
    "Depending on the time interval, different tuner parameters and goals are relevant to a calibration. If a device is turned on (e.g. `name=\"Device On\"`), the power consumption may be a goal. If it is turned off, the temperature losses may become important and, thus, parameters like heat conductivity should represent a tuner parameter.  \n",
    "We therefore need: `name`, `goals`, `tuner_parameters`. Additionally we need a time interval in which we want to compare our data `start_time` and `stop_time`. \n",
    "\n",
    "**To detect such classes, the EBC offers `EnSTats` ([Link](https://git.rwth-aachen.de/EBC/EBC_all/Optimization-and-Calibration/enstats)), a python-library to classify and cluster time-series-data. The output may be used for Calibration.**\n",
    "\n",
    "Let's declare our CalibrationClass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixcalibuha import CalibrationClass\n",
    "\n",
    "cal_class = CalibrationClass(name=\"Device On\",  # Said name of the class\n",
    "                             start_time=0, \n",
    "                             stop_time=600,\n",
    "                             goals=goals,\n",
    "                             tuner_paras=tuner_paras,\n",
    "                             # The relevant intervals are mainly useful for calibration of multiple classes. \n",
    "                             # If you specify like below, the simulation will run from 0 to 600s, but when calling\n",
    "                             # eval_difference, only the intervals 0-100 and 500-600 are relevant. \n",
    "                             relevant_intervals=[(0, 100),(500, 600)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may open and run the file under `\\examples\\cal_classes_example.py`. The example already uses a `list` to store multiple classes. For a standard calibration, you most likely will encounter multiple `CalibrationClass`es. Therefore we normally use a list of `CalibrationClass` objects. The order is not important. Internally, classes with the same names are converted into one `CalibrationClass`. Example (from the function `merge_calibration_classes`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 100), (200, 300)]\n"
     ]
    }
   ],
   "source": [
    "from aixcalibuha import merge_calibration_classes\n",
    "cal_classes = [CalibrationClass(\"on\", 0, 100),\n",
    "               CalibrationClass(\"off\", 100, 200),\n",
    "               CalibrationClass(\"on\", 200, 300)]\n",
    "merged_classes = merge_calibration_classes(cal_classes)\n",
    "# Is equal to:\n",
    "merged_classes = [CalibrationClass(\"on\", 0, 300,\n",
    "                                    relevant_intervals=[(0,100), (200,300)]),\n",
    "                  CalibrationClass(\"off\", 100, 200)]\n",
    "\n",
    "# Test:\n",
    "print(merged_classes[0].relevant_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sensanalysis'></a>\n",
    "## Sensitivity analysis: Get to know your model parameters\n",
    "\n",
    "So far we've mostly talked about calibration. An important step towards a succesful calibration is a sensititity analysis. Be it using our tool or using other applications (Dymola offers some options - see sweep parameters), you have to know which model parameters affect which output value of your model. Not only for calibration, but also for later application of the model for studies this is vital.\n",
    "\n",
    "We won't go into much detail about the theory behind a sensitivity analysis. Taken from [Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_analysis):\n",
    "> \"Sensitivity analysis is the study of how the uncertainty in the output of a mathematical model or system (numerical or otherwise) can be divided and allocated to different sources of uncertainty in its inputs\"\n",
    "\n",
    "### What do we need to perform a sensitivity analysis?\n",
    "\n",
    "Looking at the definition, we will need a **model**, **output values** and **input values**.\n",
    "The model is provided using the `simulation_api` of `ebcpy`. Output values are in our case our `goals`, because these outputs are relevant for our calibration. Input values are the tuner parameters (`tuner_paras`), as we want to know the uncertainty of each parameter on our goals.  \n",
    "Additionally, we need to specify which analysis we want to perform. We provide an increasing set of methods, look at the docstrings to know which methods are supported.\n",
    "\n",
    "For more on the methods, check: [morris](https://salib.readthedocs.io/en/latest/api.html#method-of-morris) and/or [sobol](https://salib.readthedocs.io/en/latest/api.html#sobol-sensitivity-analysis)\n",
    "\n",
    "### How do we implement it?\n",
    "\n",
    "Adapted from the file `sen_analysis_example.py` in the examples folder.\n",
    "\n",
    "**Note:** To limit the execution time of this code, we use `num_samples=2`. The results are obviously bad, but this is just the tutorial to get you familiar with syntax and output format etc. \n",
    "\n",
    "**IMPORTANT:** Dymola needs to be installed, because it is called via its `python_interface` in the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04.05.2021-14:41:43 INFO DymolaAPI: -------------------------Initializing class DymolaAPI-------------------------\n",
      "04.05.2021-14:41:43 INFO DymolaAPI: -------------------------Initializing class DymolaAPI-------------------------\n",
      "04.05.2021-14:41:43 INFO DymolaAPI: -------------------------Initializing class DymolaAPI-------------------------\n",
      "04.05.2021-14:41:43 INFO DymolaAPI: -------------------------Initializing class DymolaAPI-------------------------\n",
      "04.05.2021-14:41:46 INFO DymolaAPI: Loading Model Modelica\n",
      "04.05.2021-14:41:46 INFO DymolaAPI: Loading Model Modelica\n",
      "04.05.2021-14:41:46 INFO DymolaAPI: Loading Model Modelica\n",
      "04.05.2021-14:41:46 INFO DymolaAPI: Loading Model Modelica\n",
      "04.05.2021-14:41:46 INFO DymolaAPI: Loaded modules\n",
      "04.05.2021-14:41:46 INFO DymolaAPI: Loaded modules\n",
      "04.05.2021-14:41:46 INFO DymolaAPI: Loaded modules\n",
      "04.05.2021-14:41:46 INFO DymolaAPI: Loaded modules\n",
      "04.05.2021-14:41:46 INFO MorrisAnalyzer: Start sensitivity analysis of class: Heat up, Time-Interval: 0-200 s\n",
      "04.05.2021-14:41:46 INFO MorrisAnalyzer: Start sensitivity analysis of class: Heat up, Time-Interval: 0-200 s\n",
      "04.05.2021-14:41:46 INFO MorrisAnalyzer: Parameter variation 1 of 8\n",
      "04.05.2021-14:41:46 INFO MorrisAnalyzer: Parameter variation 1 of 8\n",
      "04.05.2021-14:41:58 INFO MorrisAnalyzer: Parameter variation 2 of 8\n",
      "04.05.2021-14:41:58 INFO MorrisAnalyzer: Parameter variation 2 of 8\n",
      "04.05.2021-14:42:01 INFO MorrisAnalyzer: Parameter variation 3 of 8\n",
      "04.05.2021-14:42:01 INFO MorrisAnalyzer: Parameter variation 3 of 8\n",
      "04.05.2021-14:42:05 INFO MorrisAnalyzer: Parameter variation 4 of 8\n",
      "04.05.2021-14:42:05 INFO MorrisAnalyzer: Parameter variation 4 of 8\n",
      "04.05.2021-14:42:08 INFO MorrisAnalyzer: Parameter variation 5 of 8\n",
      "04.05.2021-14:42:08 INFO MorrisAnalyzer: Parameter variation 5 of 8\n",
      "04.05.2021-14:42:12 INFO MorrisAnalyzer: Parameter variation 6 of 8\n",
      "04.05.2021-14:42:12 INFO MorrisAnalyzer: Parameter variation 6 of 8\n",
      "04.05.2021-14:42:15 INFO MorrisAnalyzer: Parameter variation 7 of 8\n",
      "04.05.2021-14:42:15 INFO MorrisAnalyzer: Parameter variation 7 of 8\n",
      "04.05.2021-14:42:18 INFO MorrisAnalyzer: Parameter variation 8 of 8\n",
      "04.05.2021-14:42:18 INFO MorrisAnalyzer: Parameter variation 8 of 8\n",
      "04.05.2021-14:42:21 INFO DymolaAPI: Closing Dymola\n",
      "04.05.2021-14:42:21 INFO DymolaAPI: Closing Dymola\n",
      "04.05.2021-14:42:21 INFO DymolaAPI: Closing Dymola\n",
      "04.05.2021-14:42:21 INFO DymolaAPI: Closing Dymola\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             mu_star  \\\n",
      "0  [0.1491764446140984, 1.2575638158693754, 2.712...   \n",
      "\n",
      "                                        mu_star_conf  \\\n",
      "0  [0.04916387106912529, 0.2149542846387027, 0.51...   \n",
      "\n",
      "                       names  \\\n",
      "0  [C, m_flow_2, heatConv_a]   \n",
      "\n",
      "                                               sigma  \\\n",
      "0  [0.04999672514349026, 1.7784638039521323, 0.53...   \n",
      "\n",
      "                                                  mu  duration[s]  \n",
      "0  [-0.1491764446140984, 0.15932462172930995, -2....    34.516285  \n",
      "                                                   0\n",
      "0  <aixcalibuha.CalibrationClass object at 0x0000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04.05.2021-14:42:22 INFO DymolaAPI: Successfully closed Dymola\n",
      "04.05.2021-14:42:22 INFO DymolaAPI: Successfully closed Dymola\n",
      "04.05.2021-14:42:22 INFO DymolaAPI: Successfully closed Dymola\n",
      "04.05.2021-14:42:22 INFO DymolaAPI: Successfully closed Dymola\n"
     ]
    }
   ],
   "source": [
    "from ebcpy.examples import dymola_api_example\n",
    "from aixcalibuha.examples import cal_classes_example\n",
    "from aixcalibuha.sensanalyzer import SobolAnalyzer, MorrisAnalyzer\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# %% Parameters for sen-analysis:\n",
    "# Used to call `goals.eval_difference`.\n",
    "statistical_measure = \"RMSE\"\n",
    "# Check out the ebcpy tutorial for an introduction to the simulation API.\n",
    "cd = os.path.normpath(os.path.join(os.getcwd(), \"testzone\"))\n",
    "sim_api = dymola_api_example.setup_dymola_api(cd=cd)\n",
    "# Pick out calibration classes from the example file\n",
    "cal_classes = cal_classes_example.setup_calibration_classes()[0:2]\n",
    "\n",
    "# %% Sensitivity analysis:\n",
    "# So far the methods Morris and Sobol are available options. We refer to SALib's documentation on these methods.\n",
    "# For the present case, 8 samples are generated (parameters(3) + 1) * num_samples(2) = 8\n",
    "\n",
    "sen_analyzer = MorrisAnalyzer(cd=sim_api.cd,  # cd is used for logging\n",
    "                              sim_api=sim_api,\n",
    "                              num_samples=2,\n",
    "                              statistical_measure=statistical_measure)\n",
    "\n",
    "# Evaluate and quantify which tuner parameter has which influence on which class\n",
    "sen_result = sen_analyzer.run(calibration_classes=cal_classes)\n",
    "\n",
    "for result in sen_result:\n",
    "    print(pd.DataFrame(result))\n",
    "# Close Dymola\n",
    "sim_api.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters do we now extract? This is up to you. We implemented a small function `automatic_select`, where we receive all tuner parameters below a certain `threshold` for a given `key`. Be cautious using this function. First look at the results yourself and see which threshold is a good one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_classes = sen_analyzer.automatic_select(calibration_classes=[cal_classes[0]],\n",
    "                                            result=sen_result,\n",
    "                                            threshold=1,\n",
    "                                            key=\"mu_star\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single_cal'></a>\n",
    "## Modelica calibration: Getting started on calibration\n",
    "\n",
    "The simplest type of calibration in Modelica is the single-class calibration. The `Calibrator` class of `aixcalibuha` inherites from the `Optimizer` class in `ebcpy`. This is due to the fact that a calibration is an optimization. \n",
    "\n",
    "At this point you should already know the things you need for a calibration: `tuner_parameters`, `goals` and at least one `CalibrationClass`. Furthermore, a model is necessary (hence `simulation_api`), and a `statistical_measure` to evaluate the difference between measured and simulated data. Further keyword arguments (`kwargs`) may help with a successful calibration. The next section (Advanced calibration) goes into more detail on that. Read the docstring of the classes to learn more about avaiable options. If you have questions, as always, please raise an issue.\n",
    "\n",
    "The following code is based on the example file: `aixcalibuha\\examples\\calibration_example.py`\n",
    "\n",
    "**Note:** To limit the execution time of this code, we use the solver/method/framework-specific kwargs `maxiter=0` and `popsize=1`. This limits the number of simulations drastically (to around 9 in the present case). The results are obviously bad, but this is just the tutorial to get you familiar with syntax and output format etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from ebcpy.examples import dymola_api_example\n",
    "from aixcalibuha.calibration import modelica\n",
    "from aixcalibuha.examples import cal_classes_example\n",
    "\n",
    "# Equivalent to the setup of the sensitivity analyzer:\n",
    "statistical_measure = \"RMSE\"\n",
    "cd = os.path.normpath(os.path.join(os.getcwd(), \"testzone\"))\n",
    "simulation_api = dymola_api_example.setup_dymola_api(cd=cd)\n",
    "cal_classes = cal_classes_example.setup_calibration_classes()\n",
    "# For the single-class, we only need one. Let's take the first one:\n",
    "cal_class = cal_classes[0]\n",
    "# Choose the framework (required): See the docstrings of ebcpy.optimization.optimize for available options.\n",
    "framework=\"scipy_differential_evolution\"\n",
    "# Choose solver / method (required only for some frameworks). For instance, the framework 'dlib_minimize' would not need a method.\n",
    "method=\"best1bin\"\n",
    "# More on kwargs later. This is just to limit runtime here\n",
    "# Useable kwargs are highly dependent of selected framework and solver method!\n",
    "kwargs_calibrator = {\"maxiter\":0, \"popsize\": 1}\n",
    "# The interactive plotting has some bugs in jupyter.\n",
    "kwargs_calibrator.update({\"show_plot\" : False})\n",
    "\n",
    "# Setup the class\n",
    "single_class_cal = modelica.ModelicaCalibrator(cd=simulation_api.cd,\n",
    "                                               sim_api=simulation_api,\n",
    "                                               statistical_measure=statistical_measure,\n",
    "                                               calibration_class=cal_class,\n",
    "                                               **kwargs_calibrator)\n",
    "# Start the calibration process\n",
    "single_class_cal.calibrate(framework=framework, method=method)\n",
    "# Close dymola\n",
    "simulation_api.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adv_cal'></a>\n",
    "## Advanced calibration: Multiple classes, kwargs, solvers and best practices\n",
    "\n",
    "You should now know the basics of this framework. To get into a little bit more detail, we've prepared different sections in the following. Check out any one you find interesting and want to learn more about:\n",
    "\n",
    "1. **Multiple classes**: What strategies exists for the calibration?\n",
    "2. **kwargs**: How can I change the settings of the calibrator?\n",
    "3. **Solver options**: How can I tune the optimizer to enhance the results?\n",
    "4. **Best practices**: Some general advices for getting good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mult_cal'></a>\n",
    "### 1. Multiple classes calibration\n",
    "\n",
    "Calibrating one `CalibrationClass` is a straight foreward task. You have one set of tuner parameters and one time interval. When aiming to calibrate multiple classes, different possible strategies arise to fulfill the goal of calibration. We've come up with different approaches and narrowed down to two options:\n",
    "\n",
    "<img src=\"tutorial/multiple_class_calibration.png\">\n",
    "\n",
    "Looking at the code, these two options are implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ebcpy.examples import dymola_api_example\n",
    "from aixcalibuha.calibration import modelica\n",
    "from aixcalibuha.examples import cal_classes_example\n",
    "\n",
    "# Equivalent to the setup of the single-class-calibration:\n",
    "statistical_measure = \"RMSE\"\n",
    "cd = os.path.normpath(os.path.join(os.getcwd(), \"testzone\"))\n",
    "simulation_api = dymola_api_example.setup_dymola_api(cd=cd)\n",
    "cal_classes = cal_classes_example.setup_calibration_classes()\n",
    "framework=\"scipy_differential_evolution\"\n",
    "method=\"best1bin\"\n",
    "# Choose options for multiple-class-calibration:\n",
    "start_time_method=\"timedelta\"  # Or \"fixstart\"\n",
    "# This parameter is coupled to the parameters fix_start_time and timedelta\n",
    "# For timedelta you have to pass a timedelta (e.g. timedelta=10 -> timedelta of 10 s), \n",
    "# For fixstart you have to pass a fix_start_time (e.g. fix_start_time=0 s -> start_time=0 for all classes)\n",
    "# More on kwargs later. This is just to limit runtime here\n",
    "kwargs_calibrator = {\"maxiter\":0, \"popsize\":1}\n",
    "# The interactive plotting has some bugs in jupyter.\n",
    "kwargs_calibrator.update({\"show_plot\" : False})\n",
    "\n",
    "# Setup the class\n",
    "multiple_class_cal = modelica.MultipleClassCalibrator(\n",
    "                                    cd=simulation_api.cd,\n",
    "                                    sim_api=simulation_api,\n",
    "                                    statistical_measure=statistical_measure,\n",
    "                                    calibration_classes=cal_classes,\n",
    "                                    start_time_method=start_time_method,\n",
    "                                    timedelta=0,\n",
    "                                    **kwargs_calibrator)\n",
    "# Start the calibration process\n",
    "multiple_class_cal.calibrate(framework=framework, method=method)\n",
    "# Close dymola\n",
    "simulation_api.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now open the files under `aixcalibuha/examples/testzone` and see the results of the calibration. However, as we only used a limit number of function evaluations, we refer to section \"5. Visualization: The different plots explained\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kwargs_cal'></a>\n",
    "### 2. kwargs - Settings of the calibrator:\n",
    "\n",
    "Different keyword arguments act as settings for the calibration. Although solver-specific attributes are also set using keyword arguments, we only introduce the calibration-specific keyword arguments here. See the section on solver-specific kwargs for more on that. \n",
    "\n",
    "All keyword arguments are explained in the docs. We therefore can either open the documentation ([This direct link](https://ebc.pages.rwth-aachen.de/EBC_all/Optimization-and-Calibration/AixCaliBuHA/master/docs/) is referenced in the [Documentation section of the Readme.md](https://git.rwth-aachen.de/EBC/EBC_all/Optimization-and-Calibration/AixCaliBuHA#documentation)) or look at the docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixcalibuha.calibration import modelica\n",
    "print(modelica.ModelicaCalibrator.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dict to easily setup our keyword arguments:\n",
    "# Specify values for keyword arguments to customize the Calibration process for a single-class-calibration\n",
    "kwargs_calibrator = {\"timedelta\": 0,\n",
    "                     \"save_files\": False,\n",
    "                     \"verbose_logging\": True,\n",
    "                     \"show_plot\": True,\n",
    "                     \"create_tsd_plot\": True,\n",
    "                     \"save_tsd_plot\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the multiple class calibration, additional kwargs exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelica.MultipleClassCalibrator.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use multiple classes, we may update our original dict:\n",
    "kwargs_multiple_classes = {\"merge_multiple_classes\": True,\n",
    "                           \"fix_start_time\": 0,\n",
    "                           \"timedelta\": 0}\n",
    "if len(cal_classes) > 1:\n",
    "    kwargs_calibrator.update(kwargs_multiple_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kwargs_solver'></a>\n",
    "### 3. Solver options:\n",
    "\n",
    "Introduced in the `ebcpy`-Tutorial, the following options are not directly part of AixCaliBuHA. However, some are quite useful for the use case of calibration and we will, therefore, highlight them shortly. Note that we refer to the original documentation of each framework/method/solver for a more detailed explanation of each parameter.\n",
    "\n",
    "**Note** All values below are the default values. Finding good values is up to the user. Not all keywords are necessary for all methods. \n",
    "\n",
    "Info on:\n",
    "- [scipy_differential_evolution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html)\n",
    "- [scipy_minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)\n",
    "- [dlib_minimize](http://dlib.net/python/index.html#dlib.find_min_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify solver-specific keyword-arguments depending on the solver and method you will use\n",
    "# scipy differential evolution. \n",
    "kwargs_scipy_dif_evo = {\"maxiter\": 1000,\n",
    "                        \"popsize\": 15,\n",
    "                        \"mutation\": (0.5, 1),\n",
    "                        \"recombination\": 0.7,\n",
    "                        \"seed\": None,\n",
    "                        \"polish\": True,\n",
    "                        \"init\": 'latinhypercube',\n",
    "                        \"atol\": 0}\n",
    "# Dlib: num_function_calls is the maximal number of iterations. \n",
    "# Something like 400 maybe is a good starting point\n",
    "kwargs_dlib_min = {\"num_function_calls\": int(1e9),\n",
    "                   \"solver_epsilon\":0}\n",
    "\n",
    "# Scipy-minimize:\n",
    "kwargs_scipy_min = {\"tol\": None,\n",
    "                    \"options\": None,\n",
    "                    \"constraints\": None,\n",
    "                    \"jac\": None,\n",
    "                    \"hess\": None,\n",
    "                    \"hessp\": None}\n",
    "\n",
    "# Merge the dictionaries into one.\n",
    "# If you change the solver, also change the solver-kwargs-dict in the line below. \n",
    "# In the example, a simple if-case handels this automatically\n",
    "kwargs_calibrator.update(kwargs_dlib_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best_practices'></a>\n",
    "### 4. Best practices\n",
    "\n",
    "When performing an optimization, one should know the [**no free lunch theorem**](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization). Therefore, we cannot tell you which solver is the best and which settings to choose. However, as we are limited to Building Energy Systems, the following best practices may be a good starting point for you. They are purely based on experience in calibration and are neither empirically grounded or mathematically proven. Nevertheless, here we go:\n",
    "\n",
    "- **Contribute to this list**: If you learn/know something useful about calibration which should be in this list, add it!\n",
    "- **Contribute to AixCaliBuHA**: A lot of people have the same problems you have/had. Although it consumes more time than writing a quick&dirty script on your own: Help to expand and improve this framework by contributing. Not only others will thank you in the future.\n",
    "- **Use gradient-free solvers for Modelica**: Calibrating a simulation model is a black box for the solver. Therefore, gradient-free methods (`dlib_minimize`, `scipy_differential_evolution`) yield better results than most methods of e.g. `scipy_minimize`. The latter require the gradient of a function.\n",
    "- **Check your initial values**: Using `AixCaliBuHA`, we provide visualization to help you identify possible points of improvment. One is the initialization. If the difference is big, you may either use `timedelta` for a steady-state intial condition OR use explicit start values which you write in the Modelica model. You may alter these start values through the `dymola_api`.\n",
    "- **Test different solver (and options)**: This is maybe time consuming. But try out different options and see what works best for you. If you read about a solver which works well for you, raise an isse: We can include it into the framework pretty fast.\n",
    "- **Pre-process your data**: Noisy input data will most certaintly yield bad results.\n",
    "- **Never ever copy results blindly**: The two points below further explain why. We are engineers and should always question the solution the calibration/simulation yields.\n",
    "- **Don't overfit**: Especially using stocahstical methods, you will find (after 1000 of iterations) pretty good paramteres. However keep in mind that these parameters also have to be validated\n",
    "- **Question unphysical parameteres**: You modelled a physical system but some tuned parameter makes no sense? Boundaries might have been set to broad. Check if the model or the measurement is physically coherent. If not, adapt it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visual'></a>\n",
    "### 5. Visualization: The different plots explained\n",
    "\n",
    "We provide different plots to make the process of calibration clearer to you. We will go into detail on the different plots, what they tell you and how you can enable/disable them. We refer the plot names with the file names they get.\n",
    "\n",
    "---\n",
    "\n",
    "#### objective_plot:\n",
    "<img src=\"tutorial/objective_plot.svg\">\n",
    "\n",
    "**What do we see?** The solver in use was \"scipy_differential_evolution\" using the \"best1bin\" method. After around 200 iterations, the solver begins to converge. The last 150 itertions don't yield a far better solution, it is ok to stop the calibration here. You can do this using a `KeyboardInterrupt` / `STRG + C`.\n",
    "\n",
    "**How can we enable/disable the plot?** Using the `show_plot=True` keyword argument (default is `True`)\n",
    "\n",
    "---\n",
    "\n",
    "#### tuner_parameter_plot:\n",
    "<img src=\"tutorial/tuner_parameter_plot.svg\">\n",
    "\n",
    "**What do we see?** The variation of values of the tuner parameters together with their specified boundaries (red lines). The tuner parameters vary significantly in the first 200 iterations. At convergence the values obviously also converge.\n",
    "\n",
    "**How can we enable/disable the plot?** Using the `show_plot=True` keyword argument (default is `True`)\n",
    "\n",
    "---\n",
    "\n",
    "#### tsd_plot: Created for two different classes - \"stationary\" and \"Heat up\"\n",
    "<img src=\"tutorial/tsd_plot_heat_up.svg\">\n",
    "<img src=\"tutorial/tsd_plot_stationary.svg\">\n",
    "\n",
    "**What do we see?** The measured and simulated trajectories of our selected goals. The grey part is not used for the evaluation of the objective function. As these values are `NaN`, matplotlib may interpolate linearly between the points, so don't worry if the trajectory is not logical in the grey area. Note that the inital values for the class \"stationary\" are not matching the initial values of the measured data. Even if the parameters are set properly, the objective would yield a bad result. In this case you have to adapt the inital values of your model directly in the Modelica code (see section \"Best practices\").\n",
    "\n",
    "**How can we enable/disable the plot?** Using the `create_tsd_plot=True` keyword argument for showing it each iteration, the  `save_tsd_plot=True` for saving each of these plots. (Default is `True` and `False`, respectivly.)\n",
    "\n",
    "---\n",
    "\n",
    "#### tuner_parameter_intersection_plot:\n",
    "<img src=\"tutorial/tuner_parameter_intersection_plot.svg\">\n",
    "\n",
    "**What do we see?** This plot is generated if you calibrate multiple classes **AND** different classes pyrtially have the same tuner parameters (an intersection of `tuner_paras`). In this case multiple \"best\" values arise for one tuner parameter. The plot shows the distribution of the tuner-parameters if an intersection is present. You will also be notified in the log file. In the case this plot appears, you have to decide which value to choose. If they differ greatly, you may want to either perform a sensitivity analysis to check which parameter has the biggest impact OR re-evaluate your modelling decisions. \n",
    "\n",
    "**How can we enable/disable the plot?** Using the `show_plot=True` keyword argument (default is `True`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
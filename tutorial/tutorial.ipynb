{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on AixCaliBuHA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before going any further, please note that you first should read the tutorial on `ebcpy` ([Link](https://github.com/RWTH-EBC/ebcpy)). The basic data types are explained there. Also, an example for the simulation API is made hinting on AixCaliBuHa and why you may need it.*\n",
    "\n",
    "### What is AixCaliBuHA?\n",
    "\n",
    "**Aix** (from French Aix-la-Chapelle) **Cali**bration for **Bu**ilding and **H**V**A**C Systems\n",
    "\n",
    "This framework attempts to make the process of calibrating models used in building\n",
    "and HVAC systems easier. Different sub packages help with the underlying tasks of:\n",
    "\n",
    "- Performing a **Sensitivity Analysis** to discover tuner parameters for the calibration\n",
    "- **Calibration** of given model based on the tuner parameters, the calibration classes and specified goals to evaluate the objective function of the underlying optimization\n",
    "\n",
    "### Why Calibration\n",
    "\n",
    "When modelling Building and HVAC-Systems, one wants to make valid statements about the real system. A model which represents some real-world process in a sufficient manner (e.g. in terms of supply temperature) may be useful to makes such statements. Model parameters have to be tuned so that some simulation output matches the output in the real world (e.g. some measurement). \n",
    "- **Which parameters to tune?** You as a modeler may already know important parameters that influence your model regarding a targeted output. However, performing a **sensitivity analysis** may be helpful to quantify which parameters are important.\n",
    "- **How to tune?** Manual tuning is most often still state of the art. However it is very inefficient. Therefore, the appraoch in this framework is the combination of mathematical **optimization** for calcuation of the next **tuner parameter** value and Simulation APIs for **automation of the optimization**.\n",
    "\n",
    "### Content of this Tutorial:\n",
    " 1. [The Basics: Tuner Paramateres, Goals and Calibration-Classes](#basics)\n",
    " 2. [Sensitivity Analysis: Get to know your model parameters](#sensanalysis)\n",
    " 3. [Modelica-Calibration: Getting started on calibration](#single_cal)\n",
    " 4. [Advanced Calibration: Multiple Classes, kwargs, solvers and best practices](#adv_cal)\n",
    "  1. [Multiple-Classes Calibration](#mult_cal)\n",
    "  2. [kwargs - Settings of the Calibrator](#kwargs_cal)\n",
    "  3. [Solver Options](#kwargs_solver)\n",
    "  4. [Best Practices](#best_practices)\n",
    "  5. [Visualization: The different plots explained](#visual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basics'></a>\n",
    "## The Basics: Tuner Paramaters, Goals and Calibration Classes\n",
    "\n",
    "Partly introduced in `ebcpy`, we will briefly explain the underlying `data_types`.\n",
    "\n",
    "### Tuner Parameters:\n",
    "\n",
    "All model parameters used either in a Sensitivity Analysis, an Optimization or a Calibration are at some point tuned, thus are tuner paramteres.\n",
    "Basically, a tuner parameter has a name (string), an initial value and minimal/maximal values (floats). For efficient optimization, the values are internally normalized to the range 0..1 (if min/max values (bounds) are given. This way different units (e.g. Temperature / K and Pressure / Pa) behave the same during optimization. Some solvers require boundaries some not. Initial values are not required for some global or stochastic solvers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixcalibuha import TunerParas\n",
    "tuner_paras = TunerParas(\n",
    "    # A list with the names of the parameters in the model.\n",
    "    names=[\"speedRamp.duration\", \"valveRamp.duration\"],\n",
    "    # List with initial values as floats\n",
    "    initial_values=[0.1, 0.1],\n",
    "    # List with tuples. First item is minimal, second item maximal value\n",
    "    bounds=[(0.1, 10), (0.1, 20)])\n",
    "\n",
    "# Lets look at the object (a DataFrame is internally used)\n",
    "print(tuner_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most functions (scale, descale, etc.) are used internally for automatic calibration or similiar. \n",
    "# You may find useful:\n",
    "tuner_paras.set_value(\"speedRamp.duration\", \"max\", 5)\n",
    "tuner_paras.remove_names([\"valveRamp.duration\"])\n",
    "print(tuner_paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals:\n",
    "\n",
    "Goals are used to evaluate the difference between measured and simulated data. You may want to calibrate your model based on multiple values, e.g. power consumption, supply temperature etc.\n",
    "As mentioned in `ebcpy`, we use our own **multi index `DataFrame`**. Here with the row names `Variables` and `Tags`. Looking at `Goals`, we therefore have the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebcpy import data_types, preprocessing\n",
    "from aixcalibuha import Goals\n",
    "import pathlib, os\n",
    "basepath = pathlib.Path(os.getcwd()).parent\n",
    "# First we load our measurement data as time series data from the examples folder\n",
    "path = basepath.joinpath(\"examples\", \"data\", \"PumpAndValve.hdf\")\n",
    "mtd = data_types.TimeSeriesData(path, key=\"examples\")\n",
    "\n",
    "# I refer to the docstring of the class for more information. If you have further questions, please raise an Issue.\n",
    "print(Goals.__doc__)\n",
    "\n",
    "# Recall from the doc-strings that the dict has the following structure:\n",
    "# variable_names = {VARIABLE_NAME: [MEASUREMENT_NAME, SIMULATION_NAME]}\n",
    "variable_names = {\"TCap\": [\"TCapacity\", \"heatCapacitor.T\"],\n",
    "                  \"TPipe\": {\"meas\": \"TPipe\", \"sim\": \"pipe.T\"}}\n",
    "\n",
    "goals = Goals(meas_target_data=mtd,\n",
    "              variable_names=variable_names,\n",
    "              # The statistical measure to evaluate the difference between simulated and measured data.\n",
    "              statistical_measure=\"NRMSE\",\n",
    "              # Use weightings if one of the goals is more relevant than another. A weighted sum is applied,\n",
    "              # for example with [0.9, 0.1] the total objective will be = 0.9 * objective_goal_1 + 0.1 * objective_goal_2  \n",
    "              weightings=[0.3, 0.7])\n",
    "\n",
    "print(goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assume we've run a simulation and want to add the result data to our Goals object. (This is done automatically in the Calibration process.) First we load the simulation data and analyze it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = basepath.joinpath(\"examples\", \"data\", \"PumpAndValveSimulation.hdf\")\n",
    "\n",
    "# The class data_types.TimeSeriesData can handle both *.hdf and *.mat files. \n",
    "# Latter one are the default result file format from Modelica simualtions.\n",
    "std = data_types.TimeSeriesData(path, key=\"examples\")\n",
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we may already expect some error. Why? Because our sim-target-data is using some `float`-based index but our measurement data is using `DatetimeIndex` as an index. We can solve this issue in two ways: Either convert `std` to a `DatetimeIndex` or `mtd` to a `float`-based index. Which way to choose? We agreed on the latter one, mainly because of efficiency.  \n",
    "`set_sim_target_data` is called in every iteration, therefore the conversion would take place every iteration. Iteration here means that during the subsequent calibration process the simulation is called multiple times and this always calls afterwards the `set_sim_target_data` function. Thus, processing the `std` with the default `float`-based index saves the operation of converting the index type. In contrast, `mtd` index needs to be converted only once! To display your result in the end you may use the original index again, but for the calibration, the index of `std` is used. For the conversion, we offer a preprocessing function in `ebcpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    goals.set_sim_target_data(sim_target_data=std)\n",
    "except IndexError as e:\n",
    "    print(\"IndexError:\", e)\n",
    "    \n",
    "mtd.to_float_index()\n",
    "\n",
    "goals = Goals(meas_target_data=mtd,\n",
    "              variable_names=variable_names,\n",
    "              statistical_measure=\"NRMSE\",\n",
    "              weightings=[0.3, 0.7])\n",
    "\n",
    "goals.set_sim_target_data(std)\n",
    "# Let's look at the data:\n",
    "print(\"\\n\", goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see how the sim-target-data was added. Internally, `set_relevant_time_intervals` is used to just include the data in a specific interval (the interval is provided in the `CalibrationClass` object). \n",
    "The verbose option is used for the logger to better visualize how the weightings affect the end result and which goal is performing well and which not so much.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goals.set_relevant_time_intervals([(0, 5)])\n",
    "print(goals)\n",
    "print(goals.eval_difference(verbose=True))\n",
    "# Here is the list of all avaliable statistical measures:\n",
    "from ebcpy.utils.statistics_analyzer import StatisticsAnalyzer\n",
    "help(StatisticsAnalyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Classes:\n",
    "Last but not least we need an object to define in what time interval we want to calibrate our simulation model.\n",
    "We call this time interval **Calibration Class**, not to be confused with the **python class**. A `CalibrationClass` contains everything we need to run the calibration.  \n",
    "We need to ask ourselves: **What type of `CalibrationClass` are we talking about?**  \n",
    "Depending on the time interval, different tuner parameters and goals are relevant to a calibration. If a device is turned on (e.g. `name=\"Device On\"`), the power consumption may be a goal. If it is turned off, the temperature losses may become important and, thus, parameters like heat conductivity should represent a tuner parameter.  \n",
    "We therefore need: `name`, `goals`, `tuner_parameters`. Additionally we need a time interval in which we want to compare our data `start_time` and `stop_time`. \n",
    "\n",
    "**To detect such classes, the EBC offers `EnSTats` ([Link](https://git.rwth-aachen.de/EBC/EBC_all/Optimization-and-Calibration/enstats)), a python-library to classify and cluster time-series-data. The output may be used for Calibration.**\n",
    "\n",
    "Let's declare our CalibrationClass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixcalibuha import CalibrationClass\n",
    "\n",
    "cal_class = CalibrationClass(name=\"Device On\",  # Said name of the class\n",
    "                             start_time=0, \n",
    "                             stop_time=10,\n",
    "                             goals=goals,\n",
    "                             tuner_paras=tuner_paras,\n",
    "                             # The relevant intervals are mainly useful for calibration of multiple classes. \n",
    "                             # If you specify like below, the simulation will run from 0 to 600s, but when calling\n",
    "                             # eval_difference, only the intervals 0-100 and 500-600 are relevant. \n",
    "                             relevant_intervals=[(0, 2),(5, 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may open and run the file under `\\examples\\cal_classes_example.py`. The example already uses a `list` to store multiple classes. For a standard calibration, you most likely will encounter multiple `CalibrationClass`es. Therefore we normally use a list of `CalibrationClass` objects. The order is not important. Internally, classes with the same names are converted into one `CalibrationClass`. Example (from the function `merge_calibration_classes`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixcalibuha.data_types import merge_calibration_classes\n",
    "cal_classes = [CalibrationClass(\"on\", 0, 100),\n",
    "               CalibrationClass(\"off\", 100, 200),\n",
    "               CalibrationClass(\"on\", 200, 300)]\n",
    "merged_classes = merge_calibration_classes(cal_classes)\n",
    "# Is equal to:\n",
    "merged_classes = [CalibrationClass(\"on\", 0, 300,\n",
    "                                    relevant_intervals=[(0,100), (200,300)]),\n",
    "                  CalibrationClass(\"off\", 100, 200)]\n",
    "\n",
    "# Test:\n",
    "print(merged_classes[0].relevant_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sensanalysis'></a>\n",
    "## Sensitivity analysis: Get to know your model parameters\n",
    "\n",
    "So far we've mostly talked about calibration. An important step towards a succesful calibration is a sensititity analysis. Be it using our tool or using other applications (Dymola offers some options - see sweep parameters), you have to know which model parameters affect which output value of your model. Not only for calibration, but also for later application of the model for studies this is vital.\n",
    "\n",
    "We won't go into much detail about the theory behind a sensitivity analysis. Taken from [Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_analysis):\n",
    "> \"Sensitivity analysis is the study of how the uncertainty in the output of a mathematical model or system (numerical or otherwise) can be divided and allocated to different sources of uncertainty in its inputs\"\n",
    "\n",
    "### What do we need to perform a sensitivity analysis?\n",
    "\n",
    "Looking at the definition, we will need a **model**, **output values** and **input values**.\n",
    "The model is provided using the `simulation_api` of `ebcpy`. Output values are in our case our `goals`, because these outputs are relevant for our calibration. Input values are the tuner parameters (`tuner_paras`), as we want to know the uncertainty of each parameter on our goals.  \n",
    "Additionally, we need to specify which analysis we want to perform. We provide an increasing set of methods, look at the docstrings to know which methods are supported.\n",
    "\n",
    "For more on the methods, check: [morris](https://salib.readthedocs.io/en/latest/api.html#method-of-morris) and/or [sobol](https://salib.readthedocs.io/en/latest/api.html#sobol-sensitivity-analysis)\n",
    "\n",
    "### How do we implement it?\n",
    "\n",
    "Adapted from the file `sen_analysis_example.py` in the examples folder.\n",
    "\n",
    "**Note:** To limit the execution time of this code, we use `num_samples=2`. The results are obviously bad, but this is just the tutorial to get you familiar with syntax and output format etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a function to load the simulation \n",
    "import sys\n",
    "from ebcpy import FMU_API\n",
    "\n",
    "def setup_fmu():\n",
    "    \"\"\"Setup the FMU used in all examples and tests.\"\"\"\n",
    "    example_dir = pathlib.Path(os.getcwd()).parent\n",
    "\n",
    "    if \"win\" in sys.platform:\n",
    "        model_name = example_dir.joinpath(\"examples\", \"model\", \"PumpAndValve_windows.fmu\")\n",
    "    else:\n",
    "        model_name = example_dir.joinpath(\"examples\", \"model\", \"PumpAndValve_linux.fmu\")\n",
    "\n",
    "    return FMU_API(cd=example_dir.joinpath(\"testzone\"),\n",
    "                   model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixcalibuha import SobolAnalyzer, MorrisAnalyzer\n",
    "import os\n",
    "\n",
    "# %% Parameters for sen-analysis:\n",
    "# Check out the ebcpy tutorial for an introduction to the simulation API.\n",
    "cd = os.path.normpath(os.path.join(os.getcwd(), \"testzone\"))\n",
    "sim_api = setup_fmu()\n",
    "\n",
    "# Pick out a calibration class\n",
    "cal_classes = [\n",
    "    CalibrationClass(name=\"Device On\",  # Said name of the class\n",
    "                     start_time=0, \n",
    "                     stop_time=10,\n",
    "                     goals=goals,\n",
    "                     tuner_paras=tuner_paras,\n",
    "                     relevant_intervals=[(0, 2),(5, 10)])\n",
    "]\n",
    "\n",
    "# %% Sensitivity analysis:\n",
    "# So far the methods Morris and Sobol are available options. We refer to SALib's documentation on these methods.\n",
    "# For the present case, 8 samples are generated (parameters(3) + 1) * num_samples(2) = 8\n",
    "\n",
    "sen_analyzer = SobolAnalyzer(cd=sim_api.cd,  # cd is used for logging\n",
    "                             sim_api=sim_api,\n",
    "                             num_samples=2,\n",
    "                             )\n",
    "\n",
    "# Evaluate and quantify which tuner parameter has which influence on which class\n",
    "sen_result, classes = sen_analyzer.run(calibration_classes=cal_classes)\n",
    "\n",
    "print(sen_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters do we now extract? This is up to you. We implemented a small function `automatic_select`, where we receive all tuner parameters below a certain `threshold` for a given `key`. Be cautious using this function. First look at the results yourself and see which threshold is a good one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_classes = sen_analyzer.select_by_threshold(\n",
    "    calibration_classes=cal_classes,\n",
    "    result=sen_result,\n",
    "    threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single_cal'></a>\n",
    "## Modelica calibration: Getting started on calibration\n",
    "\n",
    "The simplest type of calibration in Modelica is the single-class calibration. The `Calibrator` class of `aixcalibuha` inherites from the `Optimizer` class in `ebcpy`. This is due to the fact that a calibration is an optimization. \n",
    "\n",
    "At this point you should already know the things you need for a calibration: `tuner_parameters`, `goals` and at least one `CalibrationClass`. Furthermore, a model is necessary (hence `simulation_api`), and a `statistical_measure` to evaluate the difference between measured and simulated data. Further keyword arguments (`kwargs`) may help with a successful calibration. The next section (Advanced calibration) goes into more detail on that. Read the docstring of the classes to learn more about avaiable options. If you have questions, as always, please raise an issue.\n",
    "\n",
    "The following code is based on the example file: `aixcalibuha\\examples\\calibration_example.py`\n",
    "\n",
    "**Note:** To limit the execution time of this code, we use the solver/method/framework-specific kwargs `maxiter=0` and `popsize=1`. This limits the number of simulations drastically (to around 9 in the present case). The results are obviously bad, but this is just the tutorial to get you familiar with syntax and output format etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from aixcalibuha import Calibrator\n",
    "\n",
    "# For the single-class, we only need one. Let's take the first one:\n",
    "cal_class = cal_classes[0]\n",
    "# Choose the framework (required): See the docstrings of ebcpy.optimization.optimize for available options.\n",
    "framework=\"scipy_differential_evolution\"\n",
    "# Choose solver / method (required only for some frameworks). For instance, the framework 'dlib_minimize' would not need a method.\n",
    "method=\"best1bin\"\n",
    "# More on kwargs later. This is just to limit runtime here\n",
    "# Useable kwargs are highly dependent of selected framework and solver method!\n",
    "kwargs_calibrator = {\"maxiter\":0, \"popsize\": 1}\n",
    "# The interactive plotting has some bugs in jupyter.\n",
    "kwargs_calibrator.update({\"show_plot\" : False})\n",
    "\n",
    "# Setup the class\n",
    "single_class_cal = Calibrator(cd=sim_api.cd,\n",
    "                              sim_api=sim_api,\n",
    "                              calibration_class=cal_class,\n",
    "                              **kwargs_calibrator)\n",
    "# Start the calibration process\n",
    "single_class_cal.calibrate(framework=framework, method=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adv_cal'></a>\n",
    "## Advanced calibration: Multiple classes, kwargs, solvers and best practices\n",
    "\n",
    "You should now know the basics of this framework. To get into a little bit more detail, we've prepared different sections in the following. Check out any one you find interesting and want to learn more about:\n",
    "\n",
    "1. **Multiple classes**: What strategies exists for the calibration?\n",
    "2. **kwargs**: How can I change the settings of the calibrator?\n",
    "3. **Solver options**: How can I tune the optimizer to enhance the results?\n",
    "4. **Best practices**: Some general advices for getting good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mult_cal'></a>\n",
    "### 1. Multiple classes calibration\n",
    "\n",
    "Calibrating one `CalibrationClass` is a straight foreward task. You have one set of tuner parameters and one time interval. When aiming to calibrate multiple classes, different possible strategies arise to fulfill the goal of calibration. We've come up with different approaches and narrowed down to two options:\n",
    "\n",
    "<img src=\"tutorial/multiple_class_calibration.png\">\n",
    "\n",
    "Looking at the code, these two options are implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from aixcalibuha.calibration import MultipleClassCalibrator\n",
    "\n",
    "# Equivalent to the setup of the single-class-calibration:\n",
    "statistical_measure = \"RMSE\"\n",
    "cd = os.path.normpath(os.path.join(os.getcwd(), \"testzone\"))\n",
    "framework=\"scipy_differential_evolution\"\n",
    "method=\"best1bin\"\n",
    "# Choose options for multiple-class-calibration:\n",
    "start_time_method=\"timedelta\"  # Or \"fixstart\"\n",
    "# This parameter is coupled to the parameters fix_start_time and timedelta\n",
    "# For timedelta you have to pass a timedelta (e.g. timedelta=10 -> timedelta of 10 s), \n",
    "# For fixstart you have to pass a fix_start_time (e.g. fix_start_time=0 s -> start_time=0 for all classes)\n",
    "# More on kwargs later. This is just to limit runtime here\n",
    "kwargs_calibrator = {\"maxiter\":0, \"popsize\":1}\n",
    "# The interactive plotting has some bugs in jupyter.\n",
    "kwargs_calibrator.update({\"show_plot\" : False})\n",
    "\n",
    "# Setup the class\n",
    "multiple_class_cal = MultipleClassCalibrator(\n",
    "    cd=sim_api.cd,\n",
    "    sim_api=sim_api,\n",
    "    statistical_measure=statistical_measure,\n",
    "    calibration_classes=cal_classes,\n",
    "    start_time_method=start_time_method,\n",
    "    timedelta=0,\n",
    "    **kwargs_calibrator)\n",
    "# Start the calibration process\n",
    "multiple_class_cal.calibrate(framework=framework, method=method)\n",
    "# Close the simulation API\n",
    "sim_api.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now open the files under `aixcalibuha/examples/testzone` and see the results of the calibration. However, as we only used a limit number of function evaluations, we refer to section \"5. Visualization: The different plots explained\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kwargs_cal'></a>\n",
    "### 2. kwargs - Settings of the calibrator:\n",
    "\n",
    "Different keyword arguments act as settings for the calibration. Although solver-specific attributes are also set using keyword arguments, we only introduce the calibration-specific keyword arguments here. See the section on solver-specific kwargs for more on that. \n",
    "\n",
    "All keyword arguments are explained in the docs. We therefore can either open the documentation ([This direct link](https://ebc.pages.rwth-aachen.de/EBC_all/Optimization-and-Calibration/AixCaliBuHA/master/docs/) is referenced in the [Documentation section of the Readme.md](https://github.com/RWTH-EBC/AixCaliBuHA#documentation)) or look at the docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aixcalibuha import Calibrator, MultipleClassCalibrator\n",
    "print(Calibrator.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dict to easily setup our keyword arguments:\n",
    "# Specify values for keyword arguments to customize the Calibration process for a single-class-calibration\n",
    "kwargs_calibrator = {\"timedelta\": 0,\n",
    "                     \"save_files\": False,\n",
    "                     \"verbose_logging\": True,\n",
    "                     \"show_plot\": True,\n",
    "                     \"create_tsd_plot\": True,\n",
    "                     \"save_tsd_plot\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the multiple class calibration, additional kwargs exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MultipleClassCalibrator.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use multiple classes, we may update our original dict:\n",
    "kwargs_multiple_classes = {\"merge_multiple_classes\": True,\n",
    "                           \"fix_start_time\": 0,\n",
    "                           \"timedelta\": 0}\n",
    "if len(cal_classes) > 1:\n",
    "    kwargs_calibrator.update(kwargs_multiple_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kwargs_solver'></a>\n",
    "### 3. Solver options:\n",
    "\n",
    "Introduced in the `ebcpy`-Tutorial, the following options are not directly part of AixCaliBuHA. However, some are quite useful for the use case of calibration and we will, therefore, highlight them shortly. Note that we refer to the original documentation of each framework/method/solver for a more detailed explanation of each parameter.\n",
    "\n",
    "**Note** All values below are the default values. Finding good values is up to the user. Not all keywords are necessary for all methods. \n",
    "\n",
    "Info on:\n",
    "- [scipy_differential_evolution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html)\n",
    "- [scipy_minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)\n",
    "- [dlib_minimize](http://dlib.net/python/index.html#dlib.find_min_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify solver-specific keyword-arguments depending on the solver and method you will use\n",
    "# scipy differential evolution. \n",
    "kwargs_scipy_dif_evo = {\"maxiter\": 1000,\n",
    "                        \"popsize\": 15,\n",
    "                        \"mutation\": (0.5, 1),\n",
    "                        \"recombination\": 0.7,\n",
    "                        \"seed\": None,\n",
    "                        \"polish\": True,\n",
    "                        \"init\": 'latinhypercube',\n",
    "                        \"atol\": 0}\n",
    "# Dlib: num_function_calls is the maximal number of iterations. \n",
    "# Something like 400 maybe is a good starting point\n",
    "kwargs_dlib_min = {\"num_function_calls\": int(1e9),\n",
    "                   \"solver_epsilon\":0}\n",
    "\n",
    "# Scipy-minimize:\n",
    "kwargs_scipy_min = {\"tol\": None,\n",
    "                    \"options\": None,\n",
    "                    \"constraints\": None,\n",
    "                    \"jac\": None,\n",
    "                    \"hess\": None,\n",
    "                    \"hessp\": None}\n",
    "\n",
    "# Merge the dictionaries into one.\n",
    "# If you change the solver, also change the solver-kwargs-dict in the line below. \n",
    "# In the example, a simple if-case handels this automatically\n",
    "kwargs_calibrator.update(kwargs_dlib_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best_practices'></a>\n",
    "### 4. Best practices\n",
    "\n",
    "When performing an optimization, one should know the [**no free lunch theorem**](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization). Therefore, we cannot tell you which solver is the best and which settings to choose. However, as we are limited to Building Energy Systems, the following best practices may be a good starting point for you. They are purely based on experience in calibration and are neither empirically grounded or mathematically proven. Nevertheless, here we go:\n",
    "\n",
    "- **Contribute to this list**: If you learn/know something useful about calibration which should be in this list, add it!\n",
    "- **Contribute to AixCaliBuHA**: A lot of people have the same problems you have/had. Although it consumes more time than writing a quick&dirty script on your own: Help to expand and improve this framework by contributing. Not only others will thank you in the future.\n",
    "- **Use gradient-free solvers for Modelica**: Calibrating a simulation model is a black box for the solver. Therefore, gradient-free methods (`dlib_minimize`, `scipy_differential_evolution`) yield better results than most methods of e.g. `scipy_minimize`. The latter require the gradient of a function.\n",
    "- **Check your initial values**: Using `AixCaliBuHA`, we provide visualization to help you identify possible points of improvment. One is the initialization. If the difference is big, you may either use `timedelta` for a steady-state intial condition OR use explicit start values which you write in the Modelica model. You may alter these start values through the `dymola_api`.\n",
    "- **Test different solver (and options)**: This is maybe time consuming. But try out different options and see what works best for you. If you read about a solver which works well for you, raise an isse: We can include it into the framework pretty fast.\n",
    "- **Pre-process your data**: Noisy input data will most certaintly yield bad results.\n",
    "- **Never ever copy results blindly**: The two points below further explain why. We are engineers and should always question the solution the calibration/simulation yields.\n",
    "- **Don't overfit**: Especially using stocahstical methods, you will find (after 1000 of iterations) pretty good paramteres. However keep in mind that these parameters also have to be validated\n",
    "- **Question unphysical parameteres**: You modelled a physical system but some tuned parameter makes no sense? Boundaries might have been set to broad. Check if the model or the measurement is physically coherent. If not, adapt it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visual'></a>\n",
    "### 5. Visualization: The different plots explained\n",
    "\n",
    "We provide different plots to make the process of calibration clearer to you. We will go into detail on the different plots, what they tell you and how you can enable/disable them. We refer the plot names with the file names they get.\n",
    "\n",
    "---\n",
    "\n",
    "#### objective_plot:\n",
    "<img src=\"tutorial/objective_plot.svg\">\n",
    "\n",
    "**What do we see?** The solver in use was \"scipy_differential_evolution\" using the \"best1bin\" method. After around 200 iterations, the solver begins to converge. The last 150 itertions don't yield a far better solution, it is ok to stop the calibration here. You can do this using a `KeyboardInterrupt` / `STRG + C`.\n",
    "\n",
    "**How can we enable/disable the plot?** Using the `show_plot=True` keyword argument (default is `True`)\n",
    "\n",
    "---\n",
    "\n",
    "#### tuner_parameter_plot:\n",
    "<img src=\"tutorial/tuner_parameter_plot.svg\">\n",
    "\n",
    "**What do we see?** The variation of values of the tuner parameters together with their specified boundaries (red lines). The tuner parameters vary significantly in the first 200 iterations. At convergence the values obviously also converge.\n",
    "\n",
    "**How can we enable/disable the plot?** Using the `show_plot=True` keyword argument (default is `True`)\n",
    "\n",
    "---\n",
    "\n",
    "#### tsd_plot: Created for two different classes - \"stationary\" and \"Heat up\"\n",
    "<img src=\"tutorial/tsd_plot_heat_up.svg\">\n",
    "<img src=\"tutorial/tsd_plot_stationary.svg\">\n",
    "\n",
    "**What do we see?** The measured and simulated trajectories of our selected goals. The grey part is not used for the evaluation of the objective function. As these values are `NaN`, matplotlib may interpolate linearly between the points, so don't worry if the trajectory is not logical in the grey area. Note that the inital values for the class \"stationary\" are not matching the initial values of the measured data. Even if the parameters are set properly, the objective would yield a bad result. In this case you have to adapt the inital values of your model directly in the Modelica code (see section \"Best practices\").\n",
    "\n",
    "**How can we enable/disable the plot?** Using the `create_tsd_plot=True` keyword argument for showing it each iteration, the  `save_tsd_plot=True` for saving each of these plots. (Default is `True` and `False`, respectivly.)\n",
    "\n",
    "---\n",
    "\n",
    "#### tuner_parameter_intersection_plot:\n",
    "<img src=\"tutorial/tuner_parameter_intersection_plot.svg\">\n",
    "\n",
    "**What do we see?** This plot is generated if you calibrate multiple classes **AND** different classes pyrtially have the same tuner parameters (an intersection of `tuner_paras`). In this case multiple \"best\" values arise for one tuner parameter. The plot shows the distribution of the tuner-parameters if an intersection is present. You will also be notified in the log file. In the case this plot appears, you have to decide which value to choose. If they differ greatly, you may want to either perform a sensitivity analysis to check which parameter has the biggest impact OR re-evaluate your modelling decisions. \n",
    "\n",
    "**How can we enable/disable the plot?** Using the `show_plot=True` keyword argument (default is `True`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
